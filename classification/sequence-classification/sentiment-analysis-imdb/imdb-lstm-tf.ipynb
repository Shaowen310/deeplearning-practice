{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Modify `imdb.py` in keras library\n",
    "\n",
    "```\n",
    "-  with np.load(path) as f:\n",
    "+  with np.load(path, allow_pickle=True) as f:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraining the dataset to the top 5,000 words. Split the dataset into train (50%) and test (50%) sets.\n",
    "\n",
    "X_train, X_test 25,000 encoded word sequence lists.\n",
    "\n",
    "See [Keras dataset doc](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# X_train ndarray(25000,) of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# X_train ndarray(25000,500)\n",
    "\n",
    "SAVE_DATASET = False\n",
    "\n",
    "if SAVE_DATASET:\n",
    "    np.save('x_train.npy', X_train)\n",
    "    np.save('x_test.npy', X_test)\n",
    "    np.save('y_train.npy', y_train)\n",
    "    np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "According to Keras [doc](https://keras.io/layers/embeddings/)\n",
    "\n",
    "Embedding layer turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model.\n",
    "\n",
    "### LSTM and Dense layer\n",
    "\n",
    "According to Keras [doc]()\n",
    "\n",
    "LSTM layer\n",
    "\n",
    "units: Positive integer, dimensionality of the output space.\n",
    "\n",
    "return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False\n",
    "\n",
    "The output is the hidden state, and it contains information on previous inputs.\n",
    "\n",
    "Dense layer\n",
    "\n",
    "Transform the hidden state to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 500, 32)           160000    \n_________________________________________________________________\nlstm (LSTM)                  (None, 100)               53200     \n_________________________________________________________________\ndense (Dense)                (None, 1)                 101       \n=================================================================\nTotal params: 213,301\nTrainable params: 213,301\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "embedding_dim = 32\n",
    "hidden_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_dim, input_length=max_review_length))\n",
    "model.add(LSTM(hidden_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/3\n25000/25000 [==============================] - 17s 692us/sample - loss: 0.4906 - accuracy: 0.7595 - val_loss: 0.3265 - val_accuracy: 0.8654\nEpoch 2/3\n25000/25000 [==============================] - 14s 576us/sample - loss: 0.3047 - accuracy: 0.8762 - val_loss: 0.3337 - val_accuracy: 0.8700\nEpoch 3/3\n25000/25000 [==============================] - 14s 577us/sample - loss: 0.4280 - accuracy: 0.8255 - val_loss: 0.4275 - val_accuracy: 0.8156\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2b594ab8d88>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 81.56%\n"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Machine Learning Mastery: Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)\n",
    "\n",
    "2. [Machine Learning Mastery: The 5 Step Life-Cycle for Long Short-Term Memory Models in Keras](https://machinelearningmastery.com/5-step-life-cycle-long-short-term-memory-models-keras/) General LSTM architecture.\n",
    "\n",
    "3. [Medium: Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://medium.com/coinmonks/word-level-lstm-text-generator-creating-automatic-song-lyrics-with-neural-networks-b8a1617104fb) Learn how LSTM works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}